{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TanishqAgarwal29/DL-projects/blob/main/therapy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlxD4oyGxaYh",
        "outputId": "68306a62-eca3-4e3c-f5d6-198f4c35ec0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5udNhSJcxg8f",
        "outputId": "938db803-af73-4c7b-b29a-e61161c21e6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/gazeboi\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/gazeboi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cPkj1Qpo5DCA",
        "outputId": "2f457621-8f03-4780-f604-cbd5ef1ae47d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: dlib in /usr/local/lib/python3.10/dist-packages (19.24.2)\n",
            "Collecting face_recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting fer\n",
            "  Downloading fer-22.5.1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting face-recognition-models>=0.3.0 (from face_recognition)\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (8.1.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from face_recognition) (9.4.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.26)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fer) (3.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fer) (2.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fer) (2.32.3)\n",
            "Collecting facenet-pytorch (from fer)\n",
            "  Downloading facenet_pytorch-2.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from fer) (1.0.3)\n",
            "Collecting ffmpeg==1.4 (from fer)\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=2.0.0->fer) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=2.0.0->fer) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=2.0.0->fer) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=2.0.0->fer) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=2.0.0->fer) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras>=2.0.0->fer) (24.1)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.0)\n",
            "Collecting Pillow (from face_recognition)\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting torch<2.3.0,>=2.2.0 (from facenet-pytorch->fer)\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision<0.18.0,>=0.17.0 (from facenet-pytorch->fer)\n",
            "  Downloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fer) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fer) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fer) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fer) (2024.7.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->fer) (4.4.2)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->fer) (0.1.10)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy->fer) (2.34.2)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->fer) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fer) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fer) (2024.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy->fer) (71.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch->fer)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=2.0.0->fer) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=2.0.0->fer) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.0.0->fer) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (1.3.0)\n",
            "Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading mediapipe-0.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fer-22.5.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.0-py3-none-any.whl (32 kB)\n",
            "Downloading facenet_pytorch-2.6.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Building wheels for collected packages: ffmpeg, face-recognition-models\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6083 sha256=ddbda6a3502222da8736e996b6b83d916b3308c0cee12e7b5b9cbafc10c39df5\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566164 sha256=508a80402103820258eb10c3c68e20a17d07a178c7cdfc1d34f62b3402539517\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/eb/cf/e9eced74122b679557f597bb7c8e4c739cfcac526db1fd523d\n",
            "Successfully built ffmpeg face-recognition-models\n",
            "Installing collected packages: ffmpeg, face-recognition-models, triton, protobuf, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, sounddevice, nvidia-cusparse-cu12, nvidia-cudnn-cu12, face_recognition, nvidia-cusolver-cu12, mediapipe, torch, torchvision, facenet-pytorch, fer\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.1\n",
            "    Uninstalling triton-2.3.1:\n",
            "      Successfully uninstalled triton-2.3.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.1+cu121\n",
            "    Uninstalling torchvision-0.18.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.18.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.4 which is incompatible.\n",
            "torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 2.2.2 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-10.2.0 face-recognition-models-0.3.0 face_recognition-1.3.0 facenet-pytorch-2.6.0 fer-22.5.1 ffmpeg-1.4 mediapipe-0.10.14 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 protobuf-4.25.4 sounddevice-0.5.0 torch-2.2.2 torchvision-0.17.2 triton-2.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "3c37527dd8c145a6b6a3164111112233"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install opencv-python dlib face_recognition mediapipe fer tqdm numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import dlib\n",
        "import face_recognition\n",
        "import mediapipe as mp\n",
        "from fer import FER\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize detectors and models\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "emotion_detector = FER(mtcnn=True)\n",
        "\n",
        "# Decrease the confidence threshold for hand detection\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=4, min_detection_confidence=0.1, min_tracking_confidence=0.1)\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "def estimate_gaze(eye_landmarks):\n",
        "    try:\n",
        "        eye_center = np.mean(eye_landmarks, axis=0)\n",
        "\n",
        "        # Calculate the eye width\n",
        "        eye_width = np.linalg.norm(eye_landmarks[0] - eye_landmarks[3])\n",
        "\n",
        "        # Use the middle top and bottom landmarks for iris estimation\n",
        "        iris_top = np.mean(eye_landmarks[1:3], axis=0)\n",
        "        iris_bottom = np.mean(eye_landmarks[4:6], axis=0)\n",
        "        iris_center = np.mean([iris_top, iris_bottom], axis=0)\n",
        "\n",
        "        # Calculate gaze vector\n",
        "        gaze_vector = iris_center - eye_center\n",
        "\n",
        "        # Normalize gaze vector by eye width to account for different face sizes/distances\n",
        "        gaze_vector /= eye_width\n",
        "\n",
        "        # Adjust the vertical component of the gaze vector\n",
        "        # This helps correct for the tendency to estimate gaze as too low\n",
        "        gaze_vector[1] *= 0.5  # Reduce the vertical component\n",
        "\n",
        "        # Renormalize the gaze vector\n",
        "        gaze_vector_norm = np.linalg.norm(gaze_vector)\n",
        "        if gaze_vector_norm > 0:\n",
        "            gaze_vector /= gaze_vector_norm\n",
        "        else:\n",
        "            return None, None\n",
        "\n",
        "        return gaze_vector, tuple(map(int, eye_center))\n",
        "    except:\n",
        "        return None, None\n",
        "\n",
        "def get_gaze_direction(gaze_vector):\n",
        "    if gaze_vector is None:\n",
        "        return \"unknown\"\n",
        "    x, y = gaze_vector\n",
        "    threshold = 0.1\n",
        "    if abs(x) < threshold and abs(y) < threshold:\n",
        "        return \"center\"\n",
        "    vertical = \"up\" if y < -threshold else \"down\" if y > threshold else \"center\"\n",
        "    horizontal = \"left\" if x < -threshold else \"right\" if x > threshold else \"center\"\n",
        "    return f\"{vertical}-{horizontal}\" if vertical != \"center\" or horizontal != \"center\" else \"center\"\n",
        "\n",
        "def robust_emotion_classification(emotions, min_confidence=0.3):\n",
        "    emotion_scores = np.array(list(emotions.values())).reshape(1, -1)\n",
        "    normalized_scores = emotion_scores / np.sum(emotion_scores)\n",
        "    normalized_emotions = dict(zip(emotions.keys(), normalized_scores[0]))\n",
        "    sorted_emotions = sorted(normalized_emotions.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_emotion, top_score = sorted_emotions[0]\n",
        "    if top_score >= min_confidence:\n",
        "        return top_emotion, emotions[top_emotion]\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def calculate_engagement(prev_gaze, current_gaze, emotion, hand_movement, hand_positions, face_positions):\n",
        "    engagement = 0.5  # Start with moderate engagement\n",
        "\n",
        "    # Gaze engagement\n",
        "    if prev_gaze != current_gaze:\n",
        "        engagement += 0.1  # Increase engagement when gaze moves\n",
        "    if \"center\" in current_gaze:\n",
        "        engagement += 0.1  # Increase engagement when looking at center (possibly at each other)\n",
        "\n",
        "    # Emotion engagement\n",
        "    if emotion == \"happy\":\n",
        "        engagement += 0.2\n",
        "    elif emotion in [\"sad\", \"angry\", \"fear\"]:\n",
        "        engagement -= 0.1\n",
        "\n",
        "    # Hand movement engagement\n",
        "    if hand_movement:\n",
        "        engagement += 0.1\n",
        "\n",
        "        # Calculate the direction of hand movement relative to the other person\n",
        "        if len(hand_positions) == 2 and len(face_positions) == 2:\n",
        "            hand_center = np.mean(hand_positions, axis=0)\n",
        "            face_center = np.mean(face_positions, axis=0)\n",
        "            movement_vector = hand_center - face_center\n",
        "            distance = np.linalg.norm(movement_vector)\n",
        "\n",
        "            # Increase engagement more if hands are moving towards the other person\n",
        "            if distance < 200:  # Adjust this threshold as needed\n",
        "                engagement += 0.2 * (1 - distance / 200)  # More engagement for closer hands\n",
        "\n",
        "    return max(0, min(engagement, 1))  # Ensure engagement is between 0 and 1\n",
        "\n",
        "def identify_child_and_therapist(face_locations, frame_shape):\n",
        "    if len(face_locations) < 2:\n",
        "        return None, None\n",
        "\n",
        "    face_data = []\n",
        "    for i, (top, right, bottom, left) in enumerate(face_locations):\n",
        "        face_size = (right - left) * (bottom - top)\n",
        "        y_position = (top + bottom) / 2 / frame_shape[0]  # Normalized y-position\n",
        "        face_data.append((i, face_size, y_position))\n",
        "\n",
        "    # Sort faces primarily by size (larger face is likely the adult)\n",
        "    sorted_faces = sorted(face_data, key=lambda x: (-x[1], x[2]))\n",
        "\n",
        "    if len(sorted_faces) >= 2:\n",
        "        therapist_index = sorted_faces[0][0]\n",
        "        child_index = sorted_faces[1][0]\n",
        "        return child_index, therapist_index\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def process_frame(frame, prev_gaze_child, prev_gaze_therapist):\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    face_locations = face_recognition.face_locations(rgb_frame, model=\"cnn\", number_of_times_to_upsample=2)\n",
        "    if len(face_locations) < 2:\n",
        "        return frame, prev_gaze_child, prev_gaze_therapist, 0.5, 0.5\n",
        "\n",
        "    child_index, therapist_index = identify_child_and_therapist(face_locations, frame.shape)\n",
        "\n",
        "    if child_index is None or therapist_index is None:\n",
        "        return frame, prev_gaze_child, prev_gaze_therapist, 0.5, 0.5\n",
        "\n",
        "    child_gaze, therapist_gaze = \"unknown\", \"unknown\"\n",
        "    child_emotion, therapist_emotion = None, None\n",
        "    hand_landmarks_list = []\n",
        "    face_positions = []\n",
        "\n",
        "    for i, (top, right, bottom, left) in enumerate(face_locations):\n",
        "        face = dlib.rectangle(left, top, right, bottom)\n",
        "        landmarks = predictor(gray, face)\n",
        "        left_eye_points = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in range(36, 42)])\n",
        "        right_eye_points = np.array([(landmarks.part(n).x, landmarks.part(n).y) for n in range(42, 48)])\n",
        "\n",
        "        label = \"Child\" if i == child_index else \"Therapist\"\n",
        "        color = (0, 255, 0) if i == child_index else (0, 0, 255)\n",
        "        cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n",
        "        cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
        "\n",
        "        left_gaze, left_eye_center = estimate_gaze(left_eye_points)\n",
        "        right_gaze, right_eye_center = estimate_gaze(right_eye_points)\n",
        "\n",
        "        gaze_direction = \"unknown\"\n",
        "        if left_gaze is not None and right_gaze is not None:\n",
        "            avg_gaze = (left_gaze + right_gaze) / 2\n",
        "            gaze_direction = get_gaze_direction(avg_gaze)\n",
        "\n",
        "            cv2.arrowedLine(frame, left_eye_center, tuple(map(int, (left_eye_center[0] + left_gaze[0] * 50, left_eye_center[1] + left_gaze[1] * 50))), (0, 0, 255), 2)\n",
        "            cv2.arrowedLine(frame, right_eye_center, tuple(map(int, (right_eye_center[0] + right_gaze[0] * 50, right_eye_center[1] + right_gaze[1] * 50))), (0, 0, 255), 2)\n",
        "            cv2.putText(frame, f\"Gaze: {gaze_direction}\", (left, bottom + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "        face_image = rgb_frame[top:bottom, left:right]\n",
        "        emotion_results = emotion_detector.detect_emotions(face_image)\n",
        "\n",
        "        emotion_label = None\n",
        "        if emotion_results:\n",
        "            emotions = emotion_results[0]['emotions']\n",
        "            emotion_label, emotion_score = robust_emotion_classification(emotions, min_confidence=0.3)\n",
        "\n",
        "            if emotion_label:\n",
        "                emotion_text = f\"Emotion: {emotion_label} ({emotion_score:.2f})\"\n",
        "                cv2.putText(frame, emotion_text, (left, bottom + 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "        face_center = ((left + right) // 2, (top + bottom) // 2)\n",
        "        face_positions.append(face_center)\n",
        "\n",
        "        if i == child_index:\n",
        "            child_gaze = gaze_direction\n",
        "            child_emotion = emotion_label\n",
        "        elif i == therapist_index:\n",
        "            therapist_gaze = gaze_direction\n",
        "            therapist_emotion = emotion_label\n",
        "\n",
        "    hand_results = hands.process(rgb_frame)\n",
        "    hand_positions = []\n",
        "    if hand_results.multi_hand_landmarks:\n",
        "        for hand_landmarks in hand_results.multi_hand_landmarks:\n",
        "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "            hand_landmarks_list.append(hand_landmarks)\n",
        "\n",
        "            # Calculate average hand position\n",
        "            hand_pos = np.mean([(lm.x * frame.shape[1], lm.y * frame.shape[0]) for lm in hand_landmarks.landmark], axis=0)\n",
        "            hand_positions.append(hand_pos)\n",
        "\n",
        "    hand_movement = len(hand_landmarks_list) > 0\n",
        "\n",
        "    child_engagement = calculate_engagement(prev_gaze_child, child_gaze, child_emotion, hand_movement, hand_positions, face_positions)\n",
        "    therapist_engagement = calculate_engagement(prev_gaze_therapist, therapist_gaze, therapist_emotion, hand_movement, hand_positions, face_positions)\n",
        "\n",
        "    cv2.putText(frame, f\"Child Engagement: {child_engagement:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 0), 2)\n",
        "    cv2.putText(frame, f\"Therapist Engagement: {therapist_engagement:.2f}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 0), 2)\n",
        "\n",
        "    return frame, child_gaze, therapist_gaze, child_engagement, therapist_engagement\n",
        "\n",
        "def process_video(input_path, output_path):\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    pbar = tqdm(total=total_frames, desc=\"Processing video\")\n",
        "\n",
        "    prev_gaze_child, prev_gaze_therapist = \"unknown\", \"unknown\"\n",
        "    child_engagements, therapist_engagements = [], []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        processed_frame, child_gaze, therapist_gaze, child_engagement, therapist_engagement = process_frame(frame, prev_gaze_child, prev_gaze_therapist)\n",
        "        out.write(processed_frame)\n",
        "\n",
        "        prev_gaze_child, prev_gaze_therapist = child_gaze, therapist_gaze\n",
        "        child_engagements.append(child_engagement)\n",
        "        therapist_engagements.append(therapist_engagement)\n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Processed video saved to: {output_path}\")\n",
        "\n",
        "    avg_child_engagement = np.mean(child_engagements)\n",
        "    avg_therapist_engagement = np.mean(therapist_engagements)\n",
        "    print(f\"Average Child Engagement: {avg_child_engagement:.2f}\")\n",
        "    print(f\"Average Therapist Engagement: {avg_therapist_engagement:.2f}\")\n",
        "\n",
        "# Set paths\n",
        "input_video = \"/content/drive/MyDrive/gazeboi/test/ABA Therapy - Social Engagement.mp4\"\n",
        "output_video = \"/content/drive/MyDrive/gazeboi/processed_video.mp4\"\n",
        "\n",
        "# Process the video\n",
        "process_video(input_video, output_video)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pt0LejwzrFZ_",
        "outputId": "b0f28ac4-daff-4c4a-a615-ad739f6d05a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing video:  13%|█▎        | 280/2095 [03:14<31:38,  1.05s/it]WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
            "\n",
            "Processing video: 100%|██████████| 2095/2095 [27:01<00:00,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed video saved to: /content/drive/MyDrive/gazeboi/processed_video.mp4\n",
            "Average Child Engagement: 0.63\n",
            "Average Therapist Engagement: 0.55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}